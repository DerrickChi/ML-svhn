{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SVHN_data.pickle', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    train_dataset = tmp['train_dataset']\n",
    "    train_labels = tmp['train_labels']\n",
    "    valid_dataset = tmp['valid_dataset']\n",
    "    valid_labels = tmp['valid_labels']\n",
    "    test_dataset = tmp['test_dataset']\n",
    "    test_labels = tmp['test_labels']\n",
    "    del tmp\n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet #1\n",
    "\n",
    "## INPUT -> [[CONV -> RELU]x1 -> POOL]x2 -> DROPOUT -> FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11  # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depthC1 = 16\n",
    "depthC2 = 32\n",
    "kp=0.9\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 5))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "    # Variables.\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depthC1],stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.ones([depthC1]))\n",
    "    \n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depthC1, depthC2],stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.ones([depthC2]))\n",
    "    \n",
    "    W_o1 = tf.Variable(tf.truncated_normal([8*8*depthC2, num_labels],stddev=0.1))\n",
    "    b_o1 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o2 = tf.Variable(tf.truncated_normal([8*8*depthC2, num_labels],stddev=0.1))\n",
    "    b_o2 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o3 = tf.Variable(tf.truncated_normal([8*8*depthC2, num_labels],stddev=0.1))\n",
    "    b_o3 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o4 = tf.Variable(tf.truncated_normal([8*8*depthC2, num_labels],stddev=0.1))\n",
    "    b_o4 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o5 = tf.Variable(tf.truncated_normal([8*8*depthC2, num_labels],stddev=0.1))\n",
    "    b_o5 = tf.Variable(tf.ones([num_labels]))\n",
    "  \n",
    "    # CNN\n",
    "    def model(data, keep_prob=1, isTraining = False):    \n",
    "        #CONV\n",
    "        h_conv1 = tf.nn.conv2d(data,W_conv1, [1,1,1,1],padding='SAME', name='conv_layer1') + b_conv1\n",
    "        h_conv1 = tf.nn.relu(h_conv1)\n",
    "        h_conv1 = tf.nn.max_pool(h_conv1, [1,2,2,1], [1,2,2,1], 'SAME') \n",
    "        \n",
    "        h_conv2 = tf.nn.conv2d(h_conv1, W_conv2, [1,1,1,1], padding='SAME', name='conv_layer2') + b_conv2\n",
    "        h_conv2 = tf.nn.relu(h_conv2)\n",
    "        h_conv2 = tf.nn.max_pool(h_conv2, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv2 = tf.nn.dropout(h_conv2, keep_prob)\n",
    "        \n",
    "        #Reshape\n",
    "        shape = h_conv2.get_shape().as_list()\n",
    "        h_conv2 = tf.reshape(h_conv2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        #OUTPUT\n",
    "        logits1 = tf.matmul(h_conv2, W_o1) + b_o1\n",
    "        logits2 = tf.matmul(h_conv2, W_o2) + b_o2\n",
    "        logits3 = tf.matmul(h_conv2, W_o3) + b_o3\n",
    "        logits4 = tf.matmul(h_conv2, W_o4) + b_o4\n",
    "        logits5 = tf.matmul(h_conv2, W_o5) + b_o5\n",
    "        return tf.pack([logits1, logits2, logits3, logits4, logits5])\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, kp) \n",
    "    \n",
    "    # Define loss function.\n",
    "    with tf.name_scope(\"loss_function\") as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:,0])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:,1])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:,2])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:,3])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:,4]))\n",
    "        # Create a summary to monitor the cost function\n",
    "        tf.scalar_summary(\"loss_function\", loss)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    def eval_accuracy(predictions, labels):\n",
    "        return tf.reduce_mean( tf.reduce_min(tf.to_float(tf.equal(tf.to_int32(predictions), labels)), axis = 1))\n",
    "\n",
    "\n",
    "    # Predictions for the minibatch training, validation, and test data.\n",
    "    train_prediction = tf.transpose(tf.argmax(logits, axis = 2))\n",
    "    valid_prediction =  tf.transpose(tf.argmax(model(tf_valid_dataset), axis = 2))\n",
    "    test_prediction =  tf.transpose(tf.argmax(model(tf_test_dataset), axis = 2))\n",
    "\n",
    "    train_accuracy = eval_accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = eval_accuracy(valid_prediction, tf_valid_labels)\n",
    "    test_accuracy = eval_accuracy(test_prediction, tf_test_labels)\n",
    "    \n",
    "     # Create summaries to monitor the accuracy\n",
    "    tf.scalar_summary(\"mini-batch accuracy\", train_accuracy)\n",
    "    tf.scalar_summary(\"validation accuracy\", valid_accuracy)\n",
    "        \n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 15001\n",
    "#num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    summary_writer = tf.train.SummaryWriter(\"./logs\", session.graph)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        if (step % 500 ==0):\n",
    "            _, l, train_acc, summary_str = session.run([train_step, loss, train_accuracy, merged_summary_op], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        else:\n",
    "            _, l, train_acc = session.run([train_step, loss, train_accuracy], feed_dict=feed_dict)\n",
    "           \n",
    "        if (step % 500 == 0): \n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % (train_acc*100))\n",
    "            print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "            print('')\n",
    "            \n",
    "    print('Test accuracy: %.1f%%' % (test_accuracy.eval()*100))\n",
    "    save_path = saver.save(session, \"./ckpt_folder/CNN_trained_initialModel.ckpt\",global_step=step)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet#2\n",
    "\n",
    "## INPUT -> [[CONV -> RELU]x1 -> POOL]x4 ->DROPOUT -> [FC -> RELU]x1 ->DROPOUT-> FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11  # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depthC1 = 16\n",
    "depthC2 = 32\n",
    "depthC3 = 64\n",
    "depthC4 = 96\n",
    "depthFC1 = 128\n",
    "kp=0.9\n",
    "#beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 5))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    tf_is_training = tf.placeholder(tf.bool, name='isTraining')\n",
    "\n",
    "    # Variables.\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depthC1],stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.ones([depthC1]))\n",
    "    \n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depthC1, depthC2],stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.ones([depthC2]))\n",
    "    \n",
    "    W_conv3 = tf.Variable(tf.truncated_normal([patch_size, patch_size,depthC2, depthC3],stddev=0.1))\n",
    "    b_conv3 = tf.Variable(tf.ones([depthC3]))\n",
    "    \n",
    "    W_conv4 = tf.Variable(tf.truncated_normal([patch_size, patch_size,depthC3, depthC4],stddev=0.1))\n",
    "    b_conv4 = tf.Variable(tf.ones([depthC4]))\n",
    "\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([2*2*depthC4, depthFC1],stddev=0.1))\n",
    "    b_fc1 = tf.Variable(tf.ones([depthFC1]))\n",
    "    \n",
    "    W_o1 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o1 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o2 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o2 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o3 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o3 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o4 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o4 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o5 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o5 = tf.Variable(tf.ones([num_labels]))\n",
    "  \n",
    "    # CNN\n",
    "    def model(data, keep_prob=1, isTraining = False):    \n",
    "        #CONV\n",
    "        h_conv1 = tf.nn.conv2d(data,W_conv1, [1,1,1,1],padding='SAME', name='conv_layer1') + b_conv1\n",
    "        #h_conv1 = tf.contrib.layers.batch_norm(h_conv1, center=True, scale=True, is_training=isTraining)\n",
    "        h_conv1 = tf.nn.relu(h_conv1)\n",
    "        h_conv1 = tf.nn.lrn(h_conv1) \n",
    "        h_conv1 = tf.nn.max_pool(h_conv1, [1,2,2,1], [1,2,2,1], 'SAME') \n",
    "        \n",
    "        h_conv2 = tf.nn.conv2d(h_conv1, W_conv2, [1,1,1,1], padding='SAME', name='conv_layer2') + b_conv2 \n",
    "        #h_conv2 = tf.contrib.layers.batch_norm(h_conv2, center=True, scale=True, is_training=isTraining)\n",
    "        h_conv2 = tf.nn.relu(h_conv2)\n",
    "        h_conv2 = tf.nn.lrn(h_conv2) \n",
    "        h_conv2 = tf.nn.max_pool(h_conv2, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv3 = tf.nn.conv2d(h_conv2, W_conv3, [1,1,1,1], padding='SAME', name='conv_layer3') + b_conv3\n",
    "        #h_conv3 = tf.contrib.layers.batch_norm(h_conv3, center=True, scale=True, is_training=isTraining)\n",
    "        h_conv3 = tf.nn.relu(h_conv3)\n",
    "        h_conv3 = tf.nn.lrn(h_conv3)\n",
    "        h_conv3 = tf.nn.max_pool(h_conv3, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv4 = tf.nn.conv2d(h_conv3, W_conv4, [1,1,1,1], padding='SAME', name='conv_layer4') + b_conv4\n",
    "        #h_conv4 = tf.contrib.layers.batch_norm(h_conv4, center=True, scale=True, is_training=isTraining)\n",
    "        h_conv4 = tf.nn.relu(h_conv4)\n",
    "        h_conv4 = tf.nn.lrn(h_conv4)\n",
    "        h_conv4 = tf.nn.max_pool(h_conv4, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv4 = tf.nn.dropout(h_conv4, keep_prob)\n",
    "        \n",
    "        #Reshape\n",
    "        shape = h_conv4.get_shape().as_list()\n",
    "        h_conv4 = tf.reshape(h_conv4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        #FC\n",
    "        h_fc1 = tf.matmul(h_conv4, W_fc1) + b_fc1\n",
    "        h_fc1 = tf.nn.relu(h_fc1)\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "        #OUTPUT\n",
    "        logits1 = tf.matmul(h_fc1, W_o1) + b_o1\n",
    "        logits2 = tf.matmul(h_fc1, W_o2) + b_o2\n",
    "        logits3 = tf.matmul(h_fc1, W_o3) + b_o3\n",
    "        logits4 = tf.matmul(h_fc1, W_o4) + b_o4\n",
    "        logits5 = tf.matmul(h_fc1, W_o5) + b_o5\n",
    "        return tf.pack([logits1, logits2, logits3, logits4, logits5])\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, kp, tf_is_training) \n",
    "    \n",
    "    # Define loss function.\n",
    "    with tf.name_scope(\"loss_function\") as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:,0])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:,1])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:,2])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:,3])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:,4]))\n",
    "        # Create a summary to monitor the cost function\n",
    "        tf.scalar_summary(\"loss_function\", loss)\n",
    "\n",
    "    #Loss function with L2 Regularization with beta=0.1\n",
    "    #with tf.name_scope(\"loss_regularized_function\") as scope:\n",
    "    #    regularizers = tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2) + \\\n",
    "    #                    tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(W_conv4) + \\\n",
    "    #                    tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_o1) + \\\n",
    "    #                    tf.nn.l2_loss(W_o2) + tf.nn.l2_loss(W_o3) + \\\n",
    "    #                    tf.nn.l2_loss(W_o4) + tf.nn.l2_loss(W_o5)\n",
    "                        \n",
    "        #loss_regularized = tf.reduce_mean(loss + beta * regularizers)\n",
    "        #Create a summary to monitor the cost function\n",
    "        #tf.scalar_summary(\"loss_regularized_function\", loss_regularized)\n",
    "\n",
    "    \n",
    "    #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    #with tf.control_dependencies(update_ops):\n",
    "        #train_step = tf.train.AdagradOptimizer(0.05).minimize(loss)\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "        #train_step = tf.train.AdamOptimizer().minimize(loss_regularized)\n",
    "    \n",
    "        ##global_step = tf.Variable(0)\n",
    "        ##learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "        ##train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    def eval_accuracy(predictions, labels):\n",
    "        return tf.reduce_mean( tf.reduce_min(tf.to_float(tf.equal(tf.to_int32(predictions), labels)), axis = 1))\n",
    "\n",
    "\n",
    "    # Predictions for the mini training, validation, and test data.\n",
    "    train_prediction = tf.transpose(tf.argmax(logits, axis = 2))\n",
    "    valid_prediction =  tf.transpose(tf.argmax(model(tf_valid_dataset), axis = 2))\n",
    "    test_prediction =  tf.transpose(tf.argmax(model(tf_test_dataset), axis = 2))\n",
    "\n",
    "    train_accuracy = eval_accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = eval_accuracy(valid_prediction, tf_valid_labels)\n",
    "    test_accuracy = eval_accuracy(test_prediction, tf_test_labels)\n",
    "    \n",
    "     # Create summaries to monitor the accuracy\n",
    "    tf.scalar_summary(\"mini-batch accuracy\", train_accuracy)\n",
    "    tf.scalar_summary(\"validation accuracy\", valid_accuracy)\n",
    "        \n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    summary_writer = tf.train.SummaryWriter(\"./logs\", session.graph)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_is_training : True}\n",
    "        if (step % 500 ==0):\n",
    "            _, l, train_acc, summary_str = session.run([train_step, loss, train_accuracy, merged_summary_op], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        else:\n",
    "            _, l, train_acc = session.run([train_step, loss, train_accuracy], feed_dict=feed_dict)\n",
    "           \n",
    "        \n",
    "        #print('step: %d' % step),\n",
    "        if (step % 500 == 0):  #500\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % (train_acc*100))\n",
    "            print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "            print('')\n",
    "            \n",
    "    print('Test accuracy: %.1f%%' % (test_accuracy.eval()*100))\n",
    "    save_path = saver.save(session, \"./ckpt_folder/CNN_trained_refinedModel.ckpt\",global_step=step)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
